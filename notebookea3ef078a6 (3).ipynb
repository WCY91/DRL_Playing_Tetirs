{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T14:11:21.839803Z",
     "iopub.status.busy": "2025-04-16T14:11:21.839521Z",
     "iopub.status.idle": "2025-04-16T14:13:42.815519Z",
     "shell.execute_reply": "2025-04-16T14:13:42.814614Z",
     "shell.execute_reply.started": "2025-04-16T14:11:21.839777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.1.0)\n",
      "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.5.1+cu124)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.7.5)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
      "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
      "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.18.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (14.0.0)\n",
      "Collecting shimmy~=1.1.0 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading Shimmy-1.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (11.1.0)\n",
      "Collecting autorom~=0.6.1 (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (8.1.8)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.32.3)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.13.1)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->stable-baselines3[extra]) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->stable-baselines3[extra]) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->stable-baselines3[extra]) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->stable-baselines3[extra]) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->stable-baselines3[extra]) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20->stable-baselines3[extra]) (2.4.1)\n",
      "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.70.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13->stable-baselines3[extra])\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (2.19.1)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.1.0; extra == \"extra\"->stable-baselines3[extra]) (6.5.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->stable-baselines3[extra]) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20->stable-baselines3[extra]) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20->stable-baselines3[extra]) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20->stable-baselines3[extra]) (2024.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.6.1->autorom[accept-rom-license]~=0.6.1; extra == \"extra\"->stable-baselines3[extra]) (2025.1.31)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20->stable-baselines3[extra]) (2024.2.0)\n",
      "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446711 sha256=7e8674bb3652723d968220c220e31bb74ebed5d3f8dda55abe3cec83bee78a76\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, AutoROM.accept-rom-license, autorom, nvidia-cusolver-cu12, shimmy, ale-py\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "  Attempting uninstall: shimmy\n",
      "    Found existing installation: Shimmy 1.3.0\n",
      "    Uninstalling Shimmy-1.3.0:\n",
      "      Successfully uninstalled Shimmy-1.3.0\n",
      "  Attempting uninstall: ale-py\n",
      "    Found existing installation: ale-py 0.10.1\n",
      "    Uninstalling ale-py-0.10.1:\n",
      "      Successfully uninstalled ale-py-0.10.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.16.11 requires shimmy>=1.2.1, but you have shimmy 1.1.0 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.6.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 shimmy-1.1.0\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.33.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "--2025-04-16 14:13:42--  http://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar\n",
      "Resolving www.aiotlab.org (www.aiotlab.org)... 18.238.176.108, 18.238.176.59, 18.238.176.89, ...\n",
      "Connecting to www.aiotlab.org (www.aiotlab.org)|18.238.176.108|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar [following]\n",
      "--2025-04-16 14:13:42--  https://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar\n",
      "Connecting to www.aiotlab.org (www.aiotlab.org)|18.238.176.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3879189 (3.7M) [binary/octet-stream]\n",
      "Saving to: ‘TetrisTCPserver_v0.6.jar’\n",
      "\n",
      "TetrisTCPserver_v0. 100%[===================>]   3.70M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2025-04-16 14:13:42 (49.2 MB/s) - ‘TetrisTCPserver_v0.6.jar’ saved [3879189/3879189]\n",
      "\n",
      "✅ 檔案複製成功\n"
     ]
    }
   ],
   "source": [
    "!pip install \"stable-baselines3[extra]\"\n",
    "!pip install wandb\n",
    "!wget http://www.aiotlab.org/teaching/oop/tetris/TetrisTCPserver_v0.6.jar\n",
    "import os\n",
    "\n",
    "if os.path.exists(\"TetrisTCPserver_v0.6.jar\"):\n",
    "    print(\"✅ 檔案複製成功\")\n",
    "else:\n",
    "    print(\"❌ 檔案複製失敗\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T14:49:45.456095Z",
     "iopub.status.busy": "2025-04-16T14:49:45.455422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java started\n",
      "✅ Java server started\n",
      "⏳ 等待 Tetris TCP server 啟動中...\n",
      "✅ Java TCP server 準備完成，連線成功\n",
      "✅ PyTorch is using GPU:Client has joined the game Tesla T4\n",
      "Client has exited the game\n",
      "\n",
      "✅ 建立環境開始\n",
      "Client has exited the game\n",
      "Client has joined the game\n",
      "Address already in use (Bind failed)\n",
      "Tetris TCP server is listening at 10612\n",
      "Client has joined the game\n",
      "Using cuda device\n",
      "Model device: cuda\n",
      "Logging to ./sb3_log/DQN_11\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.856    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 152      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.683    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 334      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.528    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 497      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.256    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 32       |\n",
      "|    total_timesteps  | 783      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.0937   |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 40       |\n",
      "|    total_timesteps  | 954      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total_timesteps  | 1646     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 161      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 91       |\n",
      "|    total_timesteps  | 2102     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.457    |\n",
      "|    n_updates        | 275      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total_timesteps  | 2529     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.605    |\n",
      "|    n_updates        | 382      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total_timesteps  | 2863     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.284    |\n",
      "|    n_updates        | 465      |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack\n",
    "from IPython.display import FileLink, display, Image\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch\n",
    "# 使用 wandb 記錄訓練日誌\n",
    "import os\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# 從 Kaggle Secrets 讀取 API Token\n",
    "user_secrets = UserSecretsClient()\n",
    "WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# 設定環境變數，模擬 login\n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "# login & init\n",
    "wandb.login()\n",
    "wandb.init(project=\"tetris-training\", entity=\"t113598065-ntut-edu-tw\")\n",
    "\n",
    "log_path = \"/kaggle/working/tetris_train_log.txt\"\n",
    "\n",
    "def write_log(message):\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    print(message)\n",
    "\n",
    "import time\n",
    "\n",
    "def wait_for_tetris_server(ip=\"127.0.0.1\", port=10612, timeout=30):\n",
    "    write_log(\"⏳ 等待 Tetris TCP server 啟動中...\")\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            test_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            test_sock.settimeout(1.0)\n",
    "            test_sock.connect((ip, port))\n",
    "            test_sock.close()\n",
    "            write_log(\"✅ Java TCP server 準備完成，連線成功\")\n",
    "            break\n",
    "        except socket.error:\n",
    "            if time.time() - start_time > timeout:\n",
    "                raise TimeoutError(\"❌ 等待 Java TCP server 超時\")\n",
    "            time.sleep(0.5)\n",
    "\n",
    "# 啟動 Java Tetris server\n",
    "print(\"Java started\")\n",
    "subprocess.Popen([\"java\", \"-jar\", \"TetrisTCPserver_v0.6.jar\"])\n",
    "write_log(\"✅ Java server started\")\n",
    "wait_for_tetris_server()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ PyTorch is using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"❌ PyTorch is using CPU\")\n",
    "# ----------------------------\n",
    "# 定義 Tetris 環境 (採用老師的格式)\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, log_path=\"./reward_log.txt\", verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_path = log_path\n",
    "        self.episode_rewards = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 每次 env step 呼叫一次，但只有在 episode 結束時才記錄\n",
    "        infos = self.locals.get(\"infos\", [])\n",
    "        for info in infos:\n",
    "            if \"episode\" in info:  # VecEnv 會回傳 episode reward\n",
    "                ep_reward = info[\"episode\"][\"r\"]\n",
    "                self.episode_rewards.append(ep_reward)\n",
    "                with open(self.log_path, \"a\") as f:\n",
    "                    f.write(f\"{len(self.episode_rewards)},{ep_reward}\\n\")\n",
    "                print(f\"📈 Episode {len(self.episode_rewards)} Reward: {ep_reward}\")\n",
    "        return True\n",
    "\n",
    "class TetrisEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 20}\n",
    "    N_DISCRETE_ACTIONS = 5\n",
    "    IMG_HEIGHT = 200\n",
    "    IMG_WIDTH = 100\n",
    "    IMG_CHANNELS = 3\n",
    "\n",
    "    def __init__(self, host_ip=\"127.0.0.1\", host_port=10612):\n",
    "        super().__init__()\n",
    "        self.action_space = spaces.Discrete(self.N_DISCRETE_ACTIONS)\n",
    "        # self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84), dtype=np.uint8)\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(1, 84, 84), dtype=np.uint8)\n",
    "        self.server_ip = host_ip\n",
    "        self.server_port = host_port\n",
    "\n",
    "        self.client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.client_sock.connect((self.server_ip, self.server_port))\n",
    "\n",
    "        # 初始化 reward shaping 與統計用變數\n",
    "        self.lines_removed = 0\n",
    "        self.height = 0\n",
    "        self.holes = 0\n",
    "        self.lifetime = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.client_sock.sendall(b\"move -1\\n\")\n",
    "        elif action == 1:\n",
    "            self.client_sock.sendall(b\"move 1\\n\")\n",
    "        elif action == 2:\n",
    "            self.client_sock.sendall(b\"rotate 0\\n\")\n",
    "        elif action == 3:\n",
    "            self.client_sock.sendall(b\"rotate 1\\n\")\n",
    "        elif action == 4:\n",
    "            self.client_sock.sendall(b\"drop\\n\")\n",
    "    \n",
    "        terminated, lines, height, holes, observation = self.get_tetris_server_response(self.client_sock)\n",
    "    \n",
    "        reward = 0\n",
    "        if action == 4:\n",
    "            reward += 5\n",
    "    \n",
    "        if height > self.height:\n",
    "            reward -= (height - self.height) * 5\n",
    "    \n",
    "        if holes < self.holes:\n",
    "            reward += (self.holes - holes) * 10\n",
    "    \n",
    "        if lines > self.lines_removed:\n",
    "            reward += (lines - self.lines_removed) * 1000\n",
    "            self.lines_removed = lines\n",
    "    \n",
    "        self.height = height\n",
    "        self.holes = holes\n",
    "        self.lifetime += 1\n",
    "    \n",
    "        info = {'removed_lines': self.lines_removed, 'lifetime': self.lifetime}\n",
    "    \n",
    "        truncated = False\n",
    "    \n",
    "        # 關鍵！處理終止觀察值\n",
    "        if terminated:\n",
    "            info['terminal_observation']  = observation.copy()  \n",
    "    \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.client_sock.sendall(b\"start\\n\")\n",
    "        terminated, lines, height, holes, observation = self.get_tetris_server_response(self.client_sock)\n",
    "        # 重置統計變數\n",
    "        self.lines_removed = 0\n",
    "        self.height = 0\n",
    "        self.holes = 0\n",
    "        self.lifetime = 0\n",
    "        return observation, {}\n",
    "\n",
    "    def render(self):\n",
    "        cv2.imshow(\"Tetris\", self.last_observation)\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    def close(self):\n",
    "        self.client_sock.close()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def get_tetris_server_response(self, sock):\n",
    "        is_game_over = (sock.recv(1) == b'\\x01')\n",
    "        removed_lines = int.from_bytes(sock.recv(4), 'big')\n",
    "        height = int.from_bytes(sock.recv(4), 'big')\n",
    "        holes = int.from_bytes(sock.recv(4), 'big')\n",
    "        img_size = int.from_bytes(sock.recv(4), 'big')\n",
    "        img_png = sock.recv(img_size)\n",
    "        nparr = np.frombuffer(img_png, np.uint8)\n",
    "        np_image = cv2.imdecode(nparr, -1)\n",
    "        resized = cv2.resize(np_image, (84, 84))\n",
    "        gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        gray = np.expand_dims(gray, axis=0)  # <- 關鍵！channel-first\n",
    "        self.last_observation = gray.copy()\n",
    "        return is_game_over, removed_lines, height, holes, gray\n",
    "    \n",
    "\n",
    "        \n",
    "# 檢查環境\n",
    "print(\"✅ 建立環境開始\")\n",
    "env = TetrisEnv()\n",
    "check_env(env)\n",
    "\n",
    "# ----------------------------\n",
    "# 建立訓練環境（使用向量化、多個 env）並加入正規化與 frame stacking\n",
    "# 這部分主要用於加速並穩定訓練\n",
    "# train_env = make_vec_env(TetrisEnv, n_envs=3)\n",
    "# train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True)\n",
    "# train_env = VecFrameStack(train_env, n_stack=4, channels_order='first')\n",
    "train_env = DummyVecEnv([lambda: TetrisEnv()])\n",
    "train_env = VecFrameStack(train_env, n_stack=4, channels_order=\"first\")\n",
    "\n",
    "# ----------------------------\n",
    "# 使用 DQN 進行訓練，調整超參數以提升效能：\n",
    "# 這裡設定 buffer_size、learning_starts、target_update_interval 等參數\n",
    "model = DQN(\"CnnPolicy\", train_env, verbose=1, tensorboard_log=\"./sb3_log/\",\n",
    "            gamma=0.95,\n",
    "            learning_rate=1e-4,         # 較低的學習率有助於穩定收斂\n",
    "            buffer_size=20000,         # 經驗回放緩衝區大小\n",
    "            learning_starts=1000,       # 多少步後開始學習\n",
    "            policy_kwargs=dict(normalize_images=False),\n",
    "            target_update_interval=1000 # 目標網路更新頻率\n",
    "           )\n",
    "write_log(\"Model device: \" + str(model.device))\n",
    "# model.learn(total_timesteps=100000)  # 可根據需要延長 timesteps1000000\n",
    "reward_logger = RewardLoggerCallback(log_path=\"./reward_log.txt\")\n",
    "model.learn(total_timesteps=5000000, callback=reward_logger)\n",
    "\n",
    "# 儲存訓練後的模型（訓練完畢後可先暫停 train_env 的歸一化更新）\n",
    "train_env.training = False\n",
    "\n",
    "# ----------------------------\n",
    "# 包裝測試環境，但僅用來符合 predict 格式，取影像還是從原生環境拿\n",
    "# wrapped_test_env = make_vec_env(TetrisEnv, n_envs=1)\n",
    "# wrapped_test_env = VecNormalize(wrapped_test_env, norm_obs=False, norm_reward=False, training=False)\n",
    "# wrapped_test_env = VecFrameStack(wrapped_test_env, n_stack=4, channels_order='first')\n",
    "wrapped_test_env = DummyVecEnv([lambda: TetrisEnv()])\n",
    "wrapped_test_env = VecFrameStack(wrapped_test_env, n_stack=4, channels_order=\"first\")\n",
    "\n",
    "# 原始環境保留用來取影像\n",
    "raw_test_env = TetrisEnv()\n",
    "\n",
    "# 初始化狀態\n",
    "wrapped_obs = wrapped_test_env.reset()\n",
    "raw_obs, _ = raw_test_env.reset()\n",
    "\n",
    "frames = []\n",
    "total_test_reward = 0\n",
    "test_steps = 1000\n",
    "\n",
    "for step in range(test_steps):\n",
    "    action, _ = model.predict(wrapped_obs, deterministic=True)\n",
    "\n",
    "    # 執行動作\n",
    "    next_raw_obs, reward, done, truncated, info = raw_test_env.step(action)\n",
    "    wrapped_obs, _, _, _ = wrapped_test_env.step(action)\n",
    "\n",
    "    total_test_reward += reward\n",
    "    frames.append(np.expand_dims(raw_obs.copy(), axis=0))\n",
    "    # frames.append(raw_obs.copy())  # 儲存原始影像\n",
    "    raw_obs = next_raw_obs\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "write_log(\"Test completed: Total reward = \" + str(total_test_reward))\n",
    "\n",
    "# 將回放影像存入資料夾（依老師格式）\n",
    "replay_folder = './replay'\n",
    "if os.path.exists(replay_folder):\n",
    "    shutil.rmtree(replay_folder)\n",
    "os.makedirs(replay_folder, exist_ok=True)\n",
    "episode_folder = os.path.join(replay_folder, \"0\", \"0\")\n",
    "os.makedirs(episode_folder, exist_ok=True)\n",
    "for i, frame in enumerate(frames):\n",
    "    fname = os.path.join(episode_folder, '{:06d}.png'.format(i))\n",
    "    cv2.imwrite(fname,frame[0].squeeze())\n",
    "\n",
    "# 產生 replay GIF（最佳回放）\n",
    "filenames = sorted(glob.glob(episode_folder + '/*.png'))\n",
    "gif_images = []\n",
    "for filename in filenames:\n",
    "    gif_images.append(imageio.imread(filename))\n",
    "imageio.mimsave('replay.gif', gif_images, loop=0)\n",
    "print(\"Replay GIF saved: replay.gif\")\n",
    "display(FileLink('replay.gif'))\n",
    "\n",
    "# 將測試結果寫入 CSV（格式與老師版本一致）\n",
    "with open('tetris_best_score_test2.csv', 'w') as fs:\n",
    "    fs.write('id,removed_lines,played_steps\\n')\n",
    "    fs.write(f'0,{info[\"removed_lines\"]},{info[\"lifetime\"]}\\n')\n",
    "    fs.write(f'1,{info[\"removed_lines\"]},{info[\"lifetime\"]}\\n')\n",
    "print(\"CSV file saved: tetris_best_score_test2.csv\")\n",
    "display(FileLink('tetris_best_score_test2.csv'))\n",
    "wandb.save('tetris_best_score_test2.csv')\n",
    "\n",
    "# ----------------------------\n",
    "# 儲存最終模型（請確認將 '113598065' 替換成你的學號）\n",
    "model.save('113598065_dqn_30env_1M.zip')\n",
    "print(\"Model saved: 113598065_dqn_30env_1M.zip\")\n",
    "display(FileLink('113598065_dqn_30env_1M.zip'))\n",
    "wandb.save('113598065_dqn_30env_1M.zip')\n",
    "\n",
    "# 關閉環境\n",
    "wrapped_test_env.close()\n",
    "raw_test_env.close()\n",
    "train_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack, DummyVecEnv\n",
    "from IPython.display import FileLink, display, Image\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# --- Wandb Setup ---\n",
    "import os\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "# Import WandbCallback for SB3 integration\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# 從 Kaggle Secrets 讀取 API Token\n",
    "user_secrets = UserSecretsClient()\n",
    "WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "# 設定環境變數，模擬 login\n",
    "os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "\n",
    "# login & init\n",
    "wandb.login()\n",
    "# Start a wandb run\n",
    "run = wandb.init(\n",
    "    project=\"tetris-training-improved\", # Changed project name slightly\n",
    "    entity=\"t113598065-ntut-edu-tw\",\n",
    "    sync_tensorboard=True,  # auto-upload sb3 logs\n",
    "    monitor_gym=True,       # auto-upload videos and plots\n",
    "    save_code=True,         # save script to wandb\n",
    "    config={ # Log hyperparameters\n",
    "        \"policy_type\": \"CnnPolicy\",\n",
    "        \"total_timesteps\": 2000000, # Example: increased timesteps\n",
    "        \"env_id\": \"TetrisEnv-v1\",\n",
    "        \"gamma\": 0.99, # Increased gamma\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"buffer_size\": 300000, # Increased buffer size\n",
    "        \"learning_starts\": 10000, # Increased learning starts\n",
    "        \"target_update_interval\": 10000, # Increased target update interval\n",
    "        \"exploration_fraction\": 0.6, # Explore for 60% of training\n",
    "        \"exploration_final_eps\": 0.05, # Lower final epsilon\n",
    "        \"batch_size\": 32, # Default for DQN, can be tuned\n",
    "        \"n_stack\": 4, # Frame stacking\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "log_path = \"/kaggle/working/tetris_train_log.txt\"\n",
    "\n",
    "def write_log(message):\n",
    "    \"\"\"Appends a message to the log file and prints it.\"\"\"\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\\n\")\n",
    "    print(message)\n",
    "\n",
    "def wait_for_tetris_server(ip=\"127.0.0.1\", port=10612, timeout=60):\n",
    "    \"\"\"Waits for the Tetris TCP server to become available.\"\"\"\n",
    "    write_log(f\"⏳ 等待 Tetris TCP server 啟動中 ({ip}:{port})...\")\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as test_sock:\n",
    "                test_sock.settimeout(1.0)\n",
    "                test_sock.connect((ip, port))\n",
    "            write_log(\"✅ Java TCP server 準備完成，連線成功\")\n",
    "            break\n",
    "        except socket.error as e:\n",
    "            if time.time() - start_time > timeout:\n",
    "                raise TimeoutError(f\"❌ 等待 Java TCP server 超時 ({timeout}s)\")\n",
    "            # write_log(f\"   連接失敗 ({e}), 重試中...\") # Optional: more verbose logging\n",
    "            time.sleep(1.0) # Wait a bit longer before retrying\n",
    "\n",
    "# --- Start Java Server ---\n",
    "try:\n",
    "    write_log(\"🚀 嘗試啟動 Java Tetris server...\")\n",
    "    # Ensure the JAR file exists\n",
    "    jar_file = \"TetrisTCPserver_v0.6.jar\"\n",
    "    if not os.path.exists(jar_file):\n",
    "         write_log(f\"❌ 錯誤: 找不到 JAR 檔案 '{jar_file}'。請確保它在工作目錄中。\")\n",
    "         # Handle error appropriately, maybe exit\n",
    "         raise FileNotFoundError(f\"JAR file '{jar_file}' not found.\")\n",
    "\n",
    "    process = subprocess.Popen([\"java\", \"-jar\", jar_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    write_log(f\"✅ Java server process 啟動 (PID: {process.pid})\")\n",
    "    wait_for_tetris_server()\n",
    "except Exception as e:\n",
    "    write_log(f\"❌ 啟動或等待 Java server 時發生錯誤: {e}\")\n",
    "    # Optionally log process output if it failed\n",
    "    if 'process' in locals() and process.poll() is not None:\n",
    "        stdout, stderr = process.communicate()\n",
    "        write_log(f\"   Java Server STDOUT: {stdout.decode('utf-8', errors='ignore')}\")\n",
    "        write_log(f\"   Java Server STDERR: {stderr.decode('utf-8', errors='ignore')}\")\n",
    "    raise # Re-raise the exception to stop the script\n",
    "\n",
    "# --- Check GPU ---\n",
    "if torch.cuda.is_available():\n",
    "    write_log(f\"✅ PyTorch is using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    write_log(\"⚠️ PyTorch is using CPU. Training will be significantly slower.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 定義 Tetris 環境 (採用老師的格式, 結合獎勵機制概念)\n",
    "# ----------------------------\n",
    "class TetrisEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Tetris that interacts with a Java TCP server.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30} # Added rgb_array for potential recording\n",
    "    N_DISCRETE_ACTIONS = 5  # 0: left, 1: right, 2: rot_left, 3: rot_right, 4: drop\n",
    "    IMG_HEIGHT = 200 # Original image height from server (used for decoding)\n",
    "    IMG_WIDTH = 100  # Original image width from server (used for decoding)\n",
    "    IMG_CHANNELS = 3 # Original image channels\n",
    "    RESIZED_DIM = 84 # Dimension for resized observation\n",
    "\n",
    "    def __init__(self, host_ip=\"127.0.0.1\", host_port=10612, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.action_space = spaces.Discrete(self.N_DISCRETE_ACTIONS)\n",
    "        # Observation space: Grayscale image, channel-first (needed for CnnPolicy)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(1, self.RESIZED_DIM, self.RESIZED_DIM), # (Channels, Height, Width)\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.server_ip = host_ip\n",
    "        self.server_port = host_port\n",
    "        self.client_sock = None # Initialize socket to None\n",
    "        self._connect_socket() # Connect in init\n",
    "\n",
    "        # Reward shaping & statistics variables\n",
    "        self.current_score = 0 # Keep track of raw game score if needed\n",
    "        self.lines_removed = 0\n",
    "        self.current_height = 0\n",
    "        self.current_holes = 0\n",
    "        self.lifetime = 0\n",
    "        self.last_observation = np.zeros(self.observation_space.shape, dtype=np.uint8) # Store last obs\n",
    "\n",
    "        # --- Reward Shaping Coefficients (Tuning is crucial!) ---\n",
    "        self.reward_line_clear_coeff = 100.0 # Base reward per line squared\n",
    "        self.penalty_height_increase_coeff = 15.0 # Penalty for increasing max height\n",
    "        self.penalty_hole_increase_coeff = 25.0 # Penalty for creating new holes\n",
    "        self.penalty_step_coeff = 0.1 # Small penalty per step to encourage speed\n",
    "        self.penalty_game_over_coeff = 500.0 # Large penalty for losing\n",
    "\n",
    "        # For rendering\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "\n",
    "    def _connect_socket(self):\n",
    "        \"\"\"Establishes connection to the game server.\"\"\"\n",
    "        try:\n",
    "            # Close existing socket if any before reconnecting\n",
    "            if self.client_sock:\n",
    "                self.client_sock.close()\n",
    "            self.client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            # Set a timeout for socket operations to prevent indefinite blocking\n",
    "            self.client_sock.settimeout(10.0) # 10 seconds timeout\n",
    "            self.client_sock.connect((self.server_ip, self.server_port))\n",
    "            write_log(f\"🔌 Socket connected to {self.server_ip}:{self.server_port}\")\n",
    "        except socket.error as e:\n",
    "            write_log(f\"❌ Socket connection error during connect: {e}\")\n",
    "            raise # Re-raise to indicate failure\n",
    "\n",
    "    def _send_command(self, command: bytes):\n",
    "        \"\"\"Sends a command to the server, handles potential errors.\"\"\"\n",
    "        try:\n",
    "            self.client_sock.sendall(command)\n",
    "        except socket.timeout:\n",
    "            write_log(\"❌ Socket timeout during send.\")\n",
    "            raise ConnectionAbortedError(\"Socket timeout during send\")\n",
    "        except socket.error as e:\n",
    "            write_log(f\"❌ Socket error during send: {e}\")\n",
    "            # Attempt to reconnect on send error? Could be risky. Better to fail.\n",
    "            raise ConnectionAbortedError(f\"Socket error during send: {e}\")\n",
    "\n",
    "    def _receive_data(self, size):\n",
    "        \"\"\"Receives exactly size bytes from the server.\"\"\"\n",
    "        data = b\"\"\n",
    "        while len(data) < size:\n",
    "            try:\n",
    "                chunk = self.client_sock.recv(size - len(data))\n",
    "                if not chunk:\n",
    "                    write_log(\"❌ Socket connection broken during receive (received empty chunk).\")\n",
    "                    raise ConnectionAbortedError(\"Socket connection broken\")\n",
    "                data += chunk\n",
    "            except socket.timeout:\n",
    "                 write_log(f\"❌ Socket timeout during receive (expected {size}, got {len(data)}).\")\n",
    "                 raise ConnectionAbortedError(\"Socket timeout during receive\")\n",
    "            except socket.error as e:\n",
    "                write_log(f\"❌ Socket error during receive: {e}\")\n",
    "                raise ConnectionAbortedError(f\"Socket error during receive: {e}\")\n",
    "        return data\n",
    "\n",
    "    def get_tetris_server_response(self):\n",
    "        \"\"\"Gets state update from the Tetris server via socket.\"\"\"\n",
    "        try:\n",
    "            is_game_over_byte = self._receive_data(1)\n",
    "            is_game_over = (is_game_over_byte == b'\\x01')\n",
    "\n",
    "            removed_lines_bytes = self._receive_data(4)\n",
    "            removed_lines = int.from_bytes(removed_lines_bytes, 'big')\n",
    "\n",
    "            height_bytes = self._receive_data(4)\n",
    "            height = int.from_bytes(height_bytes, 'big')\n",
    "\n",
    "            holes_bytes = self._receive_data(4)\n",
    "            holes = int.from_bytes(holes_bytes, 'big')\n",
    "\n",
    "            img_size_bytes = self._receive_data(4)\n",
    "            img_size = int.from_bytes(img_size_bytes, 'big')\n",
    "\n",
    "            # Ensure image size is reasonable to prevent memory issues\n",
    "            if img_size <= 0 or img_size > 500000: # Set a reasonable max size (e.g., 500KB)\n",
    "                 write_log(f\"❌ Received invalid image size: {img_size}. Aborting receive.\")\n",
    "                 raise ValueError(f\"Invalid image size received: {img_size}\")\n",
    "\n",
    "            img_png = self._receive_data(img_size)\n",
    "\n",
    "            # Decode and preprocess image\n",
    "            nparr = np.frombuffer(img_png, np.uint8)\n",
    "            # Use IMREAD_COLOR to ensure 3 channels even if image is grayscale upstream\n",
    "            np_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            if np_image is None:\n",
    "                 write_log(\"❌ Failed to decode image from server response.\")\n",
    "                 # Return a default observation or raise error\n",
    "                 # Using last observation as fallback might be problematic\n",
    "                 # raise ValueError(\"Failed to decode image\")\n",
    "                 # Let's return the last known good observation and signal game over potentially\n",
    "                 return True, self.lines_removed, self.current_height, self.current_holes, self.last_observation.copy()\n",
    "\n",
    "\n",
    "            resized = cv2.resize(np_image, (self.RESIZED_DIM, self.RESIZED_DIM), interpolation=cv2.INTER_AREA)\n",
    "            gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "            # Add channel dimension: (H, W) -> (1, H, W) for PyTorch Conv2D (channel-first)\n",
    "            observation = np.expand_dims(gray, axis=0)\n",
    "            # Ensure dtype is uint8 for the observation space\n",
    "            observation = observation.astype(np.uint8)\n",
    "\n",
    "            # Store the raw BGR resized image for rendering if needed\n",
    "            self.last_raw_render_frame = resized.copy()\n",
    "            # Store the processed observation\n",
    "            self.last_observation = observation.copy()\n",
    "\n",
    "            return is_game_over, removed_lines, height, holes, observation\n",
    "\n",
    "        except ConnectionAbortedError as e:\n",
    "             write_log(f\"❌ Connection aborted while getting server response: {e}\")\n",
    "             # Attempt to reconnect? Or just end the episode. Let's end it.\n",
    "             # Return a state indicating termination and use the last valid observation\n",
    "             return True, self.lines_removed, self.current_height, self.current_holes, self.last_observation.copy()\n",
    "        except Exception as e:\n",
    "            write_log(f\"❌ Unexpected error getting server response: {e}\")\n",
    "             # End the episode on unexpected errors\n",
    "            return True, self.lines_removed, self.current_height, self.current_holes, self.last_observation.copy()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # --- Send Action ---\n",
    "        if action == 0:\n",
    "            command = b\"move -1\\n\"\n",
    "        elif action == 1:\n",
    "            command = b\"move 1\\n\"\n",
    "        elif action == 2:\n",
    "            command = b\"rotate 0\\n\" # Assuming 0 is one direction (e.g., left/ccw)\n",
    "        elif action == 3:\n",
    "            command = b\"rotate 1\\n\" # Assuming 1 is other direction (e.g., right/cw)\n",
    "        elif action == 4:\n",
    "            command = b\"drop\\n\"\n",
    "        else:\n",
    "            write_log(f\"⚠️ Invalid action received: {action}. Sending 'drop'.\")\n",
    "            command = b\"drop\\n\" # Default safe action? Or raise error?\n",
    "\n",
    "        try:\n",
    "            self._send_command(command)\n",
    "        except ConnectionAbortedError:\n",
    "            # If sending fails, the episode must end.\n",
    "            write_log(\"❌ Ending episode due to send failure in step.\")\n",
    "            terminated = True\n",
    "            # Use last known state for observation, provide zero reward.\n",
    "            observation = self.last_observation.copy()\n",
    "            reward = self.penalty_game_over_coeff * -1 # Penalize heavily\n",
    "            info = {'removed_lines': self.lines_removed, 'lifetime': self.lifetime, 'final_status': 'send_error'}\n",
    "            # SB3 expects terminal_observation in info if terminated\n",
    "            info['terminal_observation'] = observation\n",
    "            return observation, reward, terminated, False, info # terminated=True, truncated=False\n",
    "\n",
    "        # --- Get State Update ---\n",
    "        terminated, new_lines_removed, new_height, new_holes, observation = self.get_tetris_server_response()\n",
    "\n",
    "        # --- Calculate Reward ---\n",
    "        reward = 0.0\n",
    "\n",
    "        # 1. Line Clear Reward (Quadratic)\n",
    "        lines_cleared_this_step = new_lines_removed - self.lines_removed\n",
    "        if lines_cleared_this_step > 0:\n",
    "            # Quadratic reward emphasizes clearing multiple lines\n",
    "            reward += (lines_cleared_this_step ** 2) * self.reward_line_clear_coeff\n",
    "            # Bonus for Tetris (4 lines)? Optional.\n",
    "            # if lines_cleared_this_step == 4:\n",
    "            #    reward += 1000 # Large bonus\n",
    "\n",
    "        # 2. Height Increase Penalty\n",
    "        height_increase = new_height - self.current_height\n",
    "        if height_increase > 0:\n",
    "            reward -= height_increase * self.penalty_height_increase_coeff\n",
    "\n",
    "        # 3. Hole Increase Penalty\n",
    "        hole_increase = new_holes - self.current_holes\n",
    "        if hole_increase > 0:\n",
    "            reward -= hole_increase * self.penalty_hole_increase_coeff\n",
    "        # Optional: Small reward for *filling* holes?\n",
    "        # elif hole_increase < 0:\n",
    "        #    reward += abs(hole_increase) * (self.penalty_hole_increase_coeff / 2)\n",
    "\n",
    "\n",
    "        # 4. Step Penalty (encourages efficiency)\n",
    "        reward -= self.penalty_step_coeff\n",
    "\n",
    "        # 5. Game Over Penalty\n",
    "        if terminated:\n",
    "            reward -= self.penalty_game_over_coeff\n",
    "            write_log(f\"💔 Game Over! Final Lines: {new_lines_removed}, Lifetime: {self.lifetime + 1}\")\n",
    "\n",
    "\n",
    "        # --- Update Internal State ---\n",
    "        self.lines_removed = new_lines_removed\n",
    "        self.current_height = new_height\n",
    "        self.current_holes = new_holes\n",
    "        self.lifetime += 1\n",
    "\n",
    "        # --- Prepare Return Values ---\n",
    "        info = {'removed_lines': self.lines_removed, 'lifetime': self.lifetime}\n",
    "        truncated = False # We use terminated based on the game's state\n",
    "\n",
    "        # IMPORTANT: Provide terminal observation in info when terminated\n",
    "        # This is required by SB3 for correct value estimation at episode end\n",
    "        if terminated:\n",
    "            info['terminal_observation'] = observation.copy()\n",
    "            # Log final stats to wandb if needed (can also be done in callback)\n",
    "            # wandb.log({\"final/lines_removed\": self.lines_removed, \"final/lifetime\": self.lifetime})\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed) # Important for Gym compatibility\n",
    "\n",
    "        # Ensure connection is alive, reconnect if needed\n",
    "        try:\n",
    "            # Simple check: send 'start' and see if we get a response without error\n",
    "             self._send_command(b\"start\\n\")\n",
    "             # Initial state read\n",
    "             terminated, lines, height, holes, observation = self.get_tetris_server_response()\n",
    "             if terminated: # Should not be terminated on reset, indicates server issue\n",
    "                 write_log(\"⚠️ Server reported game over immediately after reset. Attempting reconnect and reset again.\")\n",
    "                 self._connect_socket() # Reconnect\n",
    "                 self._send_command(b\"start\\n\") # Try starting again\n",
    "                 terminated, lines, height, holes, observation = self.get_tetris_server_response()\n",
    "                 if terminated:\n",
    "                      write_log(\"❌ Server still terminated after reset/reconnect. Cannot proceed.\")\n",
    "                      raise RuntimeError(\"Tetris server failed to reset properly.\")\n",
    "\n",
    "        except (ConnectionAbortedError, socket.error, TimeoutError) as e:\n",
    "             write_log(f\"🔌 Connection issue during reset ({e}). Attempting reconnect...\")\n",
    "             self._connect_socket() # Re-establish connection\n",
    "             self._send_command(b\"start\\n\") # Send start command again\n",
    "             terminated, lines, height, holes, observation = self.get_tetris_server_response() # Get initial state\n",
    "             # Check again if terminated immediately\n",
    "             if terminated:\n",
    "                 write_log(\"❌ Server terminated immediately after reset/reconnect. Cannot proceed.\")\n",
    "                 raise RuntimeError(\"Tetris server failed to reset properly after reconnect.\")\n",
    "\n",
    "\n",
    "        # Reset internal statistics\n",
    "        self.lines_removed = 0\n",
    "        self.current_height = height # Initial height from server\n",
    "        self.current_holes = holes   # Initial holes from server\n",
    "        self.lifetime = 0\n",
    "        self.last_observation = observation.copy() # Store initial observation\n",
    "\n",
    "        write_log(f\"🔄 Environment Reset. Initial state: H={height}, O={holes}\")\n",
    "\n",
    "        info = {} # No extra info needed on reset by default\n",
    "        return observation, info\n",
    "\n",
    "    def render(self):\n",
    "         # Render using the stored raw frame suitable for display\n",
    "         if self.render_mode == \"human\":\n",
    "             if self.window is None:\n",
    "                 pygame.init()\n",
    "                 pygame.display.init()\n",
    "                 self.window = pygame.display.set_mode((self.RESIZED_DIM * 3, self.RESIZED_DIM * 3)) # Upscale a bit\n",
    "                 pygame.display.set_caption(\"Tetris Env\")\n",
    "             if self.clock is None:\n",
    "                 self.clock = pygame.time.Clock()\n",
    "\n",
    "             # Need a surface to display the BGR image correctly\n",
    "             # self.last_raw_render_frame should be (H, W, C) BGR\n",
    "             if hasattr(self, 'last_raw_render_frame'):\n",
    "                 # Pygame uses RGB, OpenCV uses BGR. Need conversion.\n",
    "                 render_frame_rgb = cv2.cvtColor(self.last_raw_render_frame, cv2.COLOR_BGR2RGB)\n",
    "                 # Pygame surface requires (width, height)\n",
    "                 surf = pygame.Surface((self.RESIZED_DIM, self.RESIZED_DIM))\n",
    "                 # Transpose is needed if axes are wrong, check render_frame_rgb shape\n",
    "                 pygame.surfarray.blit_array(surf, np.transpose(render_frame_rgb, (1, 0, 2)))\n",
    "                 surf = pygame.transform.scale(surf, (self.RESIZED_DIM * 3, self.RESIZED_DIM * 3)) # Scale up\n",
    "                 self.window.blit(surf, (0, 0))\n",
    "                 pygame.event.pump()\n",
    "                 self.clock.tick(self.metadata[\"render_fps\"])\n",
    "                 pygame.display.flip()\n",
    "             else:\n",
    "                 # Draw a blank screen if no frame available yet\n",
    "                  self.window.fill((0,0,0))\n",
    "                  pygame.display.flip()\n",
    "\n",
    "\n",
    "         elif self.render_mode == \"rgb_array\":\n",
    "              # Return the last processed observation (channel first) or raw render frame\n",
    "              # Returning the processed observation might be more useful if logging video\n",
    "              # return self.last_observation # Shape (1, H, W)\n",
    "              # Or return the displayable frame\n",
    "              if hasattr(self, 'last_raw_render_frame'):\n",
    "                  return cv2.cvtColor(self.last_raw_render_frame, cv2.COLOR_BGR2RGB) # Return RGB (H, W, C)\n",
    "              else:\n",
    "                  return np.zeros((self.RESIZED_DIM, self.RESIZED_DIM, 3), dtype=np.uint8) # Return black frame\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        write_log(\"🔌 Closing environment connection.\")\n",
    "        if self.client_sock:\n",
    "            try:\n",
    "                # Send a final command? Like 'quit' if the server supports it?\n",
    "                # self.client_sock.sendall(b\"quit\\n\")\n",
    "                self.client_sock.close()\n",
    "                write_log(\"   Socket closed.\")\n",
    "            except socket.error as e:\n",
    "                 write_log(f\"   Error closing socket: {e}\")\n",
    "            self.client_sock = None\n",
    "        # Close pygame window\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "            self.window = None\n",
    "            write_log(\"   Pygame window closed.\")\n",
    "\n",
    "\n",
    "# --- Environment Setup ---\n",
    "write_log(\"✅ 建立環境開始\")\n",
    "\n",
    "# Create a function to instantiate the environment\n",
    "def make_env():\n",
    "    env = TetrisEnv()\n",
    "    return env\n",
    "\n",
    "# Use DummyVecEnv for single environment interaction with the Java server\n",
    "# If you could run multiple servers on different ports, you could use SubprocVecEnv\n",
    "train_env = DummyVecEnv([make_env])\n",
    "\n",
    "# Wrap with VecFrameStack (channel-first order is important for CnnPolicy)\n",
    "train_env = VecFrameStack(train_env, n_stack=run.config[\"n_stack\"], channels_order=\"first\")\n",
    "\n",
    "# Wrap with VecNormalize, NORMALIZING REWARDS ONLY. Observation normalization\n",
    "# should ideally be handled by the policy or done carefully if needed.\n",
    "# Since policy_kwargs has normalize_images=False, we definitely don't normalize obs here.\n",
    "train_env = VecNormalize(train_env, norm_obs=False, norm_reward=True, gamma=run.config[\"gamma\"]) # Pass gamma\n",
    "\n",
    "write_log(\"   環境建立完成並已包裝 (DummyVecEnv -> VecFrameStack -> VecNormalize)\")\n",
    "\n",
    "# Check environment (optional but recommended)\n",
    "# Note: check_env doesn't work directly on VecEnv, check the base env if needed\n",
    "# check_env(make_env())\n",
    "# write_log(\"   基礎環境檢查通過\")\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Model Setup and Training\n",
    "# ----------------------------\n",
    "write_log(\"🧠 設定 DQN 模型...\")\n",
    "\n",
    "# Define DQN model with tuned hyperparameters and Wandb logging\n",
    "model = DQN(\n",
    "    run.config[\"policy_type\"], # \"CnnPolicy\"\n",
    "    train_env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=f\"/kaggle/working/runs/{run.id}\", # Log TensorBoard data for Wandb\n",
    "    gamma=run.config[\"gamma\"],\n",
    "    learning_rate=run.config[\"learning_rate\"],\n",
    "    buffer_size=run.config[\"buffer_size\"],\n",
    "    learning_starts=run.config[\"learning_starts\"],\n",
    "    batch_size=run.config[\"batch_size\"],\n",
    "    train_freq=(1, \"step\"), # Train every step\n",
    "    gradient_steps=1,       # Perform 1 gradient update per training step\n",
    "    target_update_interval=run.config[\"target_update_interval\"],\n",
    "    exploration_fraction=run.config[\"exploration_fraction\"],\n",
    "    exploration_final_eps=run.config[\"exploration_final_eps\"],\n",
    "    policy_kwargs=dict(normalize_images=False), # As per original code\n",
    "    seed=42 # Set seed for reproducibility\n",
    "    # device=\"cuda\" if torch.cuda.is_available() else \"cpu\" # Explicitly set device if needed\n",
    ")\n",
    "write_log(f\"   模型建立完成. Device: {model.device}\")\n",
    "write_log(f\"   超參數: {run.config}\")\n",
    "\n",
    "\n",
    "# Setup Wandb callback for logging SB3 metrics, gradients, etc.\n",
    "wandb_callback = WandbCallback(\n",
    "    gradient_save_freq=10000, # Save gradients every N steps\n",
    "    model_save_path=f\"/kaggle/working/models/{run.id}\", # Save model checkpoints locally\n",
    "    model_save_freq=50000, # Save model every N steps\n",
    "    log=\"all\", # Log histograms, gradients, etc.\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "\n",
    "# --- Training ---\n",
    "write_log(\"🚀 開始訓練...\")\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=run.config[\"total_timesteps\"],\n",
    "        callback=wandb_callback, # Use Wandb callback\n",
    "        log_interval=10 # Log basic stats (like reward) every 10 episodes to console/wandb\n",
    "    )\n",
    "    write_log(\"✅ 訓練完成!\")\n",
    "except Exception as e:\n",
    "     write_log(f\"❌ 訓練過程中發生錯誤: {e}\")\n",
    "     # Save model before exiting if error occurs mid-training\n",
    "     error_save_path = '/kaggle/working/113598065_dqn_error_save.zip'\n",
    "     model.save(error_save_path)\n",
    "     write_log(f\"   模型已儲存至 {error_save_path}\")\n",
    "     wandb.save(error_save_path) # Upload to Wandb\n",
    "     run.finish(exit_code=1, quiet=True) # Finish wandb run with error code\n",
    "     raise # Re-raise exception\n",
    "\n",
    "\n",
    "# --- Save Final Model ---\n",
    "# Important: Save VecNormalize statistics before saving the agent\n",
    "# These stats are needed to properly evaluate the trained agent later\n",
    "stats_path = \"/kaggle/working/vecnormalize_stats.pkl\"\n",
    "train_env.save(stats_path)\n",
    "write_log(f\"   VecNormalize 統計數據已儲存至 {stats_path}\")\n",
    "wandb.save(stats_path) # Save stats to wandb\n",
    "\n",
    "# Save the final trained model\n",
    "final_model_name = '113598065_dqn_final.zip' # Use a clear name\n",
    "final_model_path = os.path.join(\"/kaggle/working\", final_model_name)\n",
    "model.save(final_model_path)\n",
    "write_log(f\"✅ 最終模型已儲存: {final_model_path}\")\n",
    "display(FileLink(final_model_path))\n",
    "wandb.save(final_model_path) # Upload final model to Wandb\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation (Optional but recommended)\n",
    "# ----------------------------\n",
    "write_log(\"\\n🧪 開始評估訓練後的模型...\")\n",
    "\n",
    "# Create a separate evaluation environment\n",
    "eval_env = DummyVecEnv([make_env])\n",
    "# IMPORTANT: Load the SAME VecNormalize statistics saved during training\n",
    "# Use training=False to disable updates to the running stats\n",
    "eval_env = VecNormalize.load(stats_path, eval_env)\n",
    "eval_env.training = False # Do not update stats\n",
    "eval_env.norm_reward = False # Do not normalize rewards during evaluation for true score\n",
    "\n",
    "# Wrap with FrameStack, same as training\n",
    "eval_env = VecFrameStack(eval_env, n_stack=run.config[\"n_stack\"], channels_order=\"first\")\n",
    "\n",
    "# Load the trained model (optional, could just use 'model')\n",
    "# loaded_model = DQN.load(final_model_path, env=eval_env) # Load model with eval env if needed\n",
    "\n",
    "# --- Run Evaluation Episodes ---\n",
    "num_eval_episodes = 5\n",
    "total_rewards = []\n",
    "total_lines = []\n",
    "total_lifetimes = []\n",
    "all_frames = [] # Store frames for one episode's GIF\n",
    "\n",
    "for i in range(num_eval_episodes):\n",
    "    obs = eval_env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    episode_lines = 0\n",
    "    episode_lifetime = 0\n",
    "    frames = []\n",
    "    last_info = {}\n",
    "\n",
    "    while not done:\n",
    "        # Get the raw observation for rendering/saving GIF if needed\n",
    "        # Access the underlying environment's last raw frame\n",
    "        # This assumes DummyVecEnv and accesses the first (and only) env\n",
    "        # Be careful if using SubprocVecEnv\n",
    "        raw_frame = eval_env.envs[0].render(mode=\"rgb_array\")\n",
    "        if i == 0: # Only save frames for the first evaluation episode\n",
    "             frames.append(raw_frame)\n",
    "\n",
    "        # Use deterministic=True for consistent evaluation actions\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, infos = eval_env.step(action)\n",
    "\n",
    "        # Note: reward here is the unnormalized reward because eval_env.norm_reward = False\n",
    "        episode_reward += reward[0] # VecEnv returns lists\n",
    "        last_info = infos[0] # Get info from the single env\n",
    "        episode_lines = last_info.get('removed_lines', 0) # Get final lines from info\n",
    "        episode_lifetime = last_info.get('lifetime', 0)   # Get final lifetime from info\n",
    "\n",
    "        # Handle VecEnv done signal (it's an array)\n",
    "        done = done[0]\n",
    "\n",
    "\n",
    "    write_log(f\"   評估 Episode {i+1}: Reward={episode_reward:.2f}, Lines={episode_lines}, Steps={episode_lifetime}\")\n",
    "    total_rewards.append(episode_reward)\n",
    "    total_lines.append(episode_lines)\n",
    "    total_lifetimes.append(episode_lifetime)\n",
    "    if i == 0:\n",
    "        all_frames = frames # Keep frames from first episode\n",
    "\n",
    "write_log(f\"--- 評估結果 ({num_eval_episodes} episodes) ---\")\n",
    "write_log(f\"   平均 Reward: {np.mean(total_rewards):.2f} +/- {np.std(total_rewards):.2f}\")\n",
    "write_log(f\"   平均 Lines: {np.mean(total_lines):.2f} +/- {np.std(total_lines):.2f}\")\n",
    "write_log(f\"   平均 Steps: {np.mean(total_lifetimes):.2f} +/- {np.std(total_lifetimes):.2f}\")\n",
    "\n",
    "# Log evaluation metrics to Wandb\n",
    "wandb.log({\n",
    "    \"eval/mean_reward\": np.mean(total_rewards),\n",
    "    \"eval/std_reward\": np.std(total_rewards),\n",
    "    \"eval/mean_lines\": np.mean(total_lines),\n",
    "    \"eval/std_lines\": np.std(total_lines),\n",
    "    \"eval/mean_lifetime\": np.mean(total_lifetimes),\n",
    "    \"eval/std_lifetime\": np.std(total_lifetimes),\n",
    "})\n",
    "\n",
    "\n",
    "# --- Generate Replay GIF (from first evaluation episode) ---\n",
    "if all_frames:\n",
    "    gif_path = '/kaggle/working/replay_eval.gif'\n",
    "    write_log(f\"💾 正在儲存評估回放 GIF 至 {gif_path}...\")\n",
    "    try:\n",
    "        # Ensure frames are uint8\n",
    "        imageio.mimsave(gif_path, [np.array(frame).astype(np.uint8) for frame in all_frames], fps=15, loop=0)\n",
    "        write_log(\"   GIF 儲存成功.\")\n",
    "        display(FileLink(gif_path))\n",
    "        # Log the GIF to Wandb\n",
    "        wandb.log({\"eval/replay\": wandb.Video(gif_path, fps=15, format=\"gif\")})\n",
    "    except Exception as e:\n",
    "        write_log(f\"   ❌ 儲存 GIF 時發生錯誤: {e}\")\n",
    "else:\n",
    "     write_log(\"   ⚠️ 未能儲存 GIF (沒有收集到幀).\")\n",
    "\n",
    "\n",
    "# --- Save Evaluation Results CSV ---\n",
    "csv_filename = 'tetris_evaluation_scores.csv'\n",
    "csv_path = os.path.join(\"/kaggle/working\", csv_filename)\n",
    "try:\n",
    "    with open(csv_path, 'w') as fs:\n",
    "        fs.write('episode_id,removed_lines,played_steps,reward\\n')\n",
    "        # Use stats from the first episode for the format you requested\n",
    "        # Ideally, log all episodes or averages\n",
    "        fs.write(f'eval_0,{total_lines[0]},{total_lifetimes[0]},{total_rewards[0]:.2f}\\n')\n",
    "        # Add more lines if needed, e.g., for averages or best episode\n",
    "        fs.write(f'eval_avg,{np.mean(total_lines):.2f},{np.mean(total_lifetimes):.2f},{np.mean(total_rewards):.2f}\\n')\n",
    "    write_log(f\"✅ 評估分數 CSV 已儲存: {csv_path}\")\n",
    "    display(FileLink(csv_path))\n",
    "    wandb.save(csv_path) # Upload CSV to Wandb\n",
    "except Exception as e:\n",
    "    write_log(f\"   ❌ 儲存 CSV 時發生錯誤: {e}\")\n",
    "\n",
    "\n",
    "# --- Cleanup ---\n",
    "write_log(\"🧹 清理環境...\")\n",
    "eval_env.close()\n",
    "train_env.close() # Close training env as well\n",
    "# Close the Java server process if needed (might require PID management or specific server command)\n",
    "if 'process' in locals() and process.poll() is None:\n",
    "     write_log(\"   正在終止 Java server process...\")\n",
    "     process.terminate() # Ask nicely first\n",
    "     try:\n",
    "         process.wait(timeout=5) # Wait for termination\n",
    "         write_log(\"   Java server process 已終止.\")\n",
    "     except subprocess.TimeoutExpired:\n",
    "         write_log(\"   Java server 未能在 5 秒內終止, 強制結束...\")\n",
    "         process.kill() # Force kill\n",
    "         write_log(\"   Java server process 已強制結束.\")\n",
    "\n",
    "\n",
    "# Finish the Wandb run\n",
    "run.finish()\n",
    "write_log(\"✨ Wandb run finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import socket\n",
    "import cv2\n",
    "# import matplotlib.pyplot as plt # Matplotlib not strictly needed for core logic\n",
    "import subprocess\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import imageio\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "# from stable_baselines3.common.env_checker import check_env # Not used on VecEnv\n",
    "from stable_baselines3 import DQN\n",
    "# from stable_baselines3.common.env_util import make_vec_env # Not used\n",
    "from stable_baselines3.common.vec_env import VecNormalize, VecFrameStack, DummyVecEnv\n",
    "from IPython.display import FileLink, display # Image not used directly\n",
    "# from stable_baselines3.common.callbacks import BaseCallback # Replaced by WandbCallback\n",
    "import torch\n",
    "import time\n",
    "import pygame # Added for rendering in TetrisEnv\n",
    "\n",
    "# --- Wandb Setup ---\n",
    "import os\n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "# Import WandbCallback for SB3 integration\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "# --- Configuration ---\n",
    "# Set your student ID here for filenames\n",
    "STUDENT_ID = \"113598065\"\n",
    "# Set total training steps\n",
    "#TOTAL_TIMESTEPS = 2000000 # Adjust as needed (e.g., 1M, 2M, 5M)\n",
    "TOTAL_TIMESTEPS = 2000000 # Reduced for a potentially quicker test run, increase for full training\n",
    "\n",
    "\n",
    "# --- Wandb Login and Initialization ---\n",
    "try:\n",
    "    user_secrets = UserSecretsClient()\n",
    "    WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
    "    os.environ[\"WANDB_API_KEY\"] = WANDB_API_KEY\n",
    "    wandb.login()\n",
    "    wandb_enabled = True\n",
    "except Exception as e:\n",
    "    print(f\"Wandb login failed (running without secrets?): {e}. Running without Wandb logging.\")\n",
    "    wandb_enabled = False\n",
    "    WANDB_API_KEY = None # Ensure it's None if not available\n",
    "\n",
    "# Start a wandb run if enabled\n",
    "if wandb_enabled:\n",
    "    run = wandb.init(\n",
    "        project=\"tetris-training-improved\",\n",
    "        entity=\"t113598065-ntut-edu-tw\", # Replace with your Wandb entity if different\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "        config={ # Log hyperparameters\n",
    "            \"policy_type\": \"CnnPolicy\",\n",
    "            \"total_timesteps\": TOTAL_TIMESTEPS,\n",
    "            \"env_id\": \"TetrisEnv-v1\",\n",
    "            \"gamma\": 0.99,\n",
    "            \"learning_rate\": 1e-4,\n",
    "            \"buffer_size\": 300000, # Increased buffer size\n",
    "            \"learning_starts\": 10000, # Keep reasonable starts\n",
    "            \"target_update_interval\": 10000, # Keep reasonable update interval\n",
    "            \"exploration_fraction\": 0.3, # Explore for 60% of training\n",
    "            \"exploration_final_eps\": 0.05, # Lower final epsilon\n",
    "            \"batch_size\": 32, # Default for DQN, can be tuned\n",
    "            \"n_stack\": 4, # Frame stacking\n",
    "            \"student_id\": STUDENT_ID,\n",
    "        }\n",
    "    )\n",
    "    run_id = run.id # Get run ID for saving paths\n",
    "else:\n",
    "    run = None # Set run to None if wandb is disabled\n",
    "    run_id = f\"local_{int(time.time())}\" # Create a local ID for paths\n",
    "\n",
    "\n",
    "log_path = f\"/kaggle/working/tetris_train_log_{run_id}.txt\"\n",
    "\n",
    "def write_log(message):\n",
    "    \"\"\"Appends a message to the log file and prints it.\"\"\"\n",
    "    timestamp = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    log_message = f\"{timestamp} - {message}\"\n",
    "    try:\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(log_message + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to log file {log_path}: {e}\")\n",
    "    print(log_message)\n",
    "\n",
    "def wait_for_tetris_server(ip=\"127.0.0.1\", port=10612, timeout=60):\n",
    "    \"\"\"Waits for the Tetris TCP server to become available.\"\"\"\n",
    "    write_log(f\"⏳ 等待 Tetris TCP server 啟動中 ({ip}:{port})...\")\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as test_sock:\n",
    "                test_sock.settimeout(1.0)\n",
    "                test_sock.connect((ip, port))\n",
    "            write_log(\"✅ Java TCP server 準備完成，連線成功\")\n",
    "            return True # Indicate success\n",
    "        except socket.error as e:\n",
    "            if time.time() - start_time > timeout:\n",
    "                write_log(f\"❌ 等待 Java TCP server 超時 ({timeout}s)\")\n",
    "                return False # Indicate failure\n",
    "            time.sleep(1.0) # Wait a bit longer before retrying\n",
    "\n",
    "# --- Start Java Server ---\n",
    "java_process = None # Initialize to None\n",
    "try:\n",
    "    write_log(\"🚀 嘗試啟動 Java Tetris server...\")\n",
    "    jar_file = \"TetrisTCPserver_v0.6.jar\"\n",
    "    if not os.path.exists(jar_file):\n",
    "         write_log(f\"❌ 錯誤: 找不到 JAR 檔案 '{jar_file}'。請確保它在工作目錄中。\")\n",
    "         raise FileNotFoundError(f\"JAR file '{jar_file}' not found.\")\n",
    "\n",
    "    # Start process, redirect stdout/stderr to DEVNULL if desired to keep console clean\n",
    "    java_process = subprocess.Popen(\n",
    "        [\"java\", \"-jar\", jar_file],\n",
    "        stdout=subprocess.DEVNULL, # Optional: hide server stdout\n",
    "        stderr=subprocess.DEVNULL  # Optional: hide server stderr\n",
    "    )\n",
    "    write_log(f\"✅ Java server process 啟動 (PID: {java_process.pid})\")\n",
    "    if not wait_for_tetris_server():\n",
    "        raise TimeoutError(\"Java server did not become available.\") # Raise specific error\n",
    "\n",
    "except Exception as e:\n",
    "    write_log(f\"❌ 啟動或等待 Java server 時發生錯誤: {e}\")\n",
    "    # Attempt to terminate if process started but failed connection\n",
    "    if java_process and java_process.poll() is None:\n",
    "         write_log(\"   嘗試終止未成功連接的 Java server process...\")\n",
    "         java_process.terminate()\n",
    "         try:\n",
    "             java_process.wait(timeout=2)\n",
    "         except subprocess.TimeoutExpired:\n",
    "             java_process.kill()\n",
    "    raise # Re-raise the exception to stop the script\n",
    "\n",
    "# --- Check GPU ---\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    write_log(f\"✅ PyTorch is using GPU: {device_name}\")\n",
    "    # Optional: Check compute capability if needed\n",
    "    # cc = torch.cuda.get_device_capability(0)\n",
    "    # write_log(f\"   Compute Capability: {cc[0]}.{cc[1]}\")\n",
    "else:\n",
    "    write_log(\"⚠️ PyTorch is using CPU. Training will be significantly slower.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 定義 Tetris 環境 (採用老師的格式, 結合獎勵機制概念)\n",
    "# ----------------------------\n",
    "class TetrisEnv(gym.Env):\n",
    "    \"\"\"Custom Environment for Tetris that interacts with a Java TCP server.\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n",
    "    N_DISCRETE_ACTIONS = 5\n",
    "    IMG_HEIGHT = 200\n",
    "    IMG_WIDTH = 100\n",
    "    IMG_CHANNELS = 3\n",
    "    RESIZED_DIM = 84\n",
    "\n",
    "    def __init__(self, host_ip=\"127.0.0.1\", host_port=10612, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.action_space = spaces.Discrete(self.N_DISCRETE_ACTIONS)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=255,\n",
    "            shape=(1, self.RESIZED_DIM, self.RESIZED_DIM), # (Channels, Height, Width)\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        self.server_ip = host_ip\n",
    "        self.server_port = host_port\n",
    "        self.client_sock = None\n",
    "        self._connect_socket() # Connect in init\n",
    "\n",
    "        # Reward shaping & statistics variables\n",
    "        self.lines_removed = 0\n",
    "        self.current_height = 0\n",
    "        self.current_holes = 0\n",
    "        self.lifetime = 0\n",
    "        self.last_observation = np.zeros(self.observation_space.shape, dtype=np.uint8)\n",
    "\n",
    "        # --- Reward Shaping Coefficients (TUNING REQUIRED) ---\n",
    "        self.reward_line_clear_coeff = 100.0\n",
    "        self.penalty_height_increase_coeff = 15.0\n",
    "        self.penalty_hole_increase_coeff = 25.0\n",
    "        self.penalty_step_coeff = 0.1\n",
    "        self.penalty_game_over_coeff = 500.0\n",
    "\n",
    "        # For rendering\n",
    "        self.window_surface = None\n",
    "        self.clock = None\n",
    "        self.is_pygame_initialized = False # Track Pygame init state\n",
    "\n",
    "    def _initialize_pygame(self):\n",
    "        \"\"\"Initializes Pygame if not already done.\"\"\"\n",
    "        if not self.is_pygame_initialized and self.render_mode == \"human\":\n",
    "            try:\n",
    "                import pygame\n",
    "                pygame.init()\n",
    "                pygame.display.init()\n",
    "                # Scale window for better visibility\n",
    "                window_width = self.RESIZED_DIM * 4\n",
    "                window_height = self.RESIZED_DIM * 4\n",
    "                self.window_surface = pygame.display.set_mode((window_width, window_height))\n",
    "                pygame.display.set_caption(f\"Tetris Env ({self.server_ip}:{self.server_port})\")\n",
    "                self.clock = pygame.time.Clock()\n",
    "                self.is_pygame_initialized = True\n",
    "                write_log(\"   Pygame initialized for rendering.\")\n",
    "            except ImportError:\n",
    "                write_log(\"⚠️ Pygame not installed, cannot use 'human' render mode.\")\n",
    "                self.render_mode = None # Disable human rendering\n",
    "            except Exception as e:\n",
    "                write_log(f\"⚠️ Error initializing Pygame: {e}\")\n",
    "                self.render_mode = None\n",
    "\n",
    "\n",
    "    def _connect_socket(self):\n",
    "        \"\"\"Establishes connection to the game server.\"\"\"\n",
    "        try:\n",
    "            if self.client_sock:\n",
    "                self.client_sock.close()\n",
    "            self.client_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "            self.client_sock.settimeout(10.0)\n",
    "            self.client_sock.connect((self.server_ip, self.server_port))\n",
    "            # write_log(f\"🔌 Socket connected to {self.server_ip}:{self.server_port}\") # Less verbose\n",
    "        except socket.error as e:\n",
    "            write_log(f\"❌ Socket connection error during connect: {e}\")\n",
    "            raise ConnectionError(f\"Failed to connect to Tetris server at {self.server_ip}:{self.server_port}\")\n",
    "\n",
    "    def _send_command(self, command: bytes):\n",
    "        \"\"\"Sends a command to the server, handles potential errors.\"\"\"\n",
    "        if not self.client_sock:\n",
    "             raise ConnectionError(\"Socket is not connected. Cannot send command.\")\n",
    "        try:\n",
    "            self.client_sock.sendall(command)\n",
    "        except socket.timeout:\n",
    "            write_log(\"❌ Socket timeout during send.\")\n",
    "            raise ConnectionAbortedError(\"Socket timeout during send\")\n",
    "        except socket.error as e:\n",
    "            write_log(f\"❌ Socket error during send: {e}\")\n",
    "            raise ConnectionAbortedError(f\"Socket error during send: {e}\")\n",
    "\n",
    "    def _receive_data(self, size):\n",
    "        \"\"\"Receives exactly size bytes from the server.\"\"\"\n",
    "        if not self.client_sock:\n",
    "             raise ConnectionError(\"Socket is not connected. Cannot receive data.\")\n",
    "        data = b\"\"\n",
    "        try:\n",
    "            self.client_sock.settimeout(10.0) # Set timeout for recv\n",
    "            while len(data) < size:\n",
    "                chunk = self.client_sock.recv(size - len(data))\n",
    "                if not chunk:\n",
    "                    write_log(\"❌ Socket connection broken during receive (received empty chunk).\")\n",
    "                    raise ConnectionAbortedError(\"Socket connection broken\")\n",
    "                data += chunk\n",
    "        except socket.timeout:\n",
    "             write_log(f\"❌ Socket timeout during receive (expected {size}, got {len(data)}).\")\n",
    "             raise ConnectionAbortedError(\"Socket timeout during receive\")\n",
    "        except socket.error as e:\n",
    "            write_log(f\"❌ Socket error during receive: {e}\")\n",
    "            raise ConnectionAbortedError(f\"Socket error during receive: {e}\")\n",
    "        return data\n",
    "\n",
    "    def get_tetris_server_response(self):\n",
    "        \"\"\"Gets state update from the Tetris server via socket.\"\"\"\n",
    "        try:\n",
    "            is_game_over_byte = self._receive_data(1)\n",
    "            is_game_over = (is_game_over_byte == b'\\x01')\n",
    "\n",
    "            removed_lines_bytes = self._receive_data(4)\n",
    "            removed_lines = int.from_bytes(removed_lines_bytes, 'big')\n",
    "\n",
    "            height_bytes = self._receive_data(4)\n",
    "            height = int.from_bytes(height_bytes, 'big')\n",
    "\n",
    "            holes_bytes = self._receive_data(4)\n",
    "            holes = int.from_bytes(holes_bytes, 'big')\n",
    "\n",
    "            img_size_bytes = self._receive_data(4)\n",
    "            img_size = int.from_bytes(img_size_bytes, 'big')\n",
    "\n",
    "            if img_size <= 0 or img_size > 1000000: # Increased max size slightly\n",
    "                 write_log(f\"❌ Received invalid image size: {img_size}. Aborting receive.\")\n",
    "                 raise ValueError(f\"Invalid image size received: {img_size}\")\n",
    "\n",
    "            img_png = self._receive_data(img_size)\n",
    "\n",
    "            # Decode and preprocess image\n",
    "            nparr = np.frombuffer(img_png, np.uint8)\n",
    "            np_image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "            if np_image is None:\n",
    "                 write_log(\"❌ Failed to decode image from server response.\")\n",
    "                 # Return last known state and signal termination\n",
    "                 return True, self.lines_removed, self.current_height, self.current_holes, self.last_observation.copy()\n",
    "\n",
    "            resized = cv2.resize(np_image, (self.RESIZED_DIM, self.RESIZED_DIM), interpolation=cv2.INTER_AREA)\n",
    "            gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "            observation = np.expand_dims(gray, axis=0).astype(np.uint8) # Combine steps\n",
    "\n",
    "            # Store frames for rendering/observation\n",
    "            self.last_raw_render_frame = resized.copy() # Store BGR for render\n",
    "            self.last_observation = observation.copy() # Store processed obs\n",
    "\n",
    "            return is_game_over, removed_lines, height, holes, observation\n",
    "\n",
    "        except (ConnectionAbortedError, ConnectionRefusedError, ValueError) as e:\n",
    "             write_log(f\"❌ Connection/Value error getting server response: {e}. Ending episode.\")\n",
    "             # Return last known state and signal termination\n",
    "             return True, self.lines_removed, self.current_height, self.current_holes, self.last_observation.copy()\n",
    "        except Exception as e:\n",
    "            write_log(f\"❌ Unexpected error getting server response: {e}. Ending episode.\")\n",
    "            # Return last known state and signal termination\n",
    "            return True, self.lines_removed, self.current_height, self.current_holes, self.last_observation.copy()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # --- Send Action ---\n",
    "        command_map = {\n",
    "            0: b\"move -1\\n\", 1: b\"move 1\\n\",\n",
    "            2: b\"rotate 0\\n\", 3: b\"rotate 1\\n\",\n",
    "            4: b\"drop\\n\"\n",
    "        }\n",
    "        command = command_map.get(action)\n",
    "        if command is None:\n",
    "            write_log(f\"⚠️ Invalid action received: {action}. Sending 'drop'.\")\n",
    "            command = b\"drop\\n\"\n",
    "\n",
    "        try:\n",
    "            self._send_command(command)\n",
    "        except (ConnectionAbortedError, ConnectionError) as e:\n",
    "            write_log(f\"❌ Ending episode due to send failure in step: {e}\")\n",
    "            terminated = True\n",
    "            observation = self.last_observation.copy()\n",
    "            reward = self.penalty_game_over_coeff * -1\n",
    "            info = {'removed_lines': self.lines_removed, 'lifetime': self.lifetime, 'final_status': 'send_error'}\n",
    "            info['terminal_observation'] = observation\n",
    "            return observation, reward, terminated, False, info\n",
    "\n",
    "        # --- Get State Update ---\n",
    "        terminated, new_lines_removed, new_height, new_holes, observation = self.get_tetris_server_response()\n",
    "\n",
    "        # --- Calculate Reward ---\n",
    "        reward = 0.0\n",
    "        lines_cleared_this_step = new_lines_removed - self.lines_removed\n",
    "        if lines_cleared_this_step > 0:\n",
    "            reward += (lines_cleared_this_step ** 2) * self.reward_line_clear_coeff\n",
    "\n",
    "        height_increase = new_height - self.current_height\n",
    "        if height_increase > 0:\n",
    "            reward -= height_increase * self.penalty_height_increase_coeff\n",
    "\n",
    "        hole_increase = new_holes - self.current_holes\n",
    "        if hole_increase > 0:\n",
    "            reward -= hole_increase * self.penalty_hole_increase_coeff\n",
    "\n",
    "        reward -= self.penalty_step_coeff # Step penalty\n",
    "\n",
    "        if terminated:\n",
    "            reward -= self.penalty_game_over_coeff\n",
    "            # Log only once per game over for clarity\n",
    "            write_log(f\"💔 Game Over! Final Lines: {new_lines_removed}, Lifetime: {self.lifetime + 1}, reward: {reward}\")\n",
    "\n",
    "        # --- Update Internal State ---\n",
    "        self.lines_removed = new_lines_removed\n",
    "        self.current_height = new_height\n",
    "        self.current_holes = new_holes\n",
    "        self.lifetime += 1\n",
    "\n",
    "        # --- Prepare Return Values ---\n",
    "        info = {'removed_lines': self.lines_removed, 'lifetime': self.lifetime}\n",
    "        truncated = False\n",
    "\n",
    "        if terminated:\n",
    "            info['terminal_observation'] = observation.copy()\n",
    "            # Log final stats here if needed, or use SB3 logger/callback\n",
    "            # Example: print(f\"Episode End: Lines={self.lines_removed}, Lifetime={self.lifetime}, Reward={reward}\")\n",
    "\n",
    "\n",
    "        # Optional: Render on step if requested\n",
    "        if self.render_mode == \"human\":\n",
    "             self.render()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        for attempt in range(3): # Allow a few attempts to reset/reconnect\n",
    "            try:\n",
    "                self._send_command(b\"start\\n\")\n",
    "                terminated, lines, height, holes, observation = self.get_tetris_server_response()\n",
    "                if terminated:\n",
    "                    write_log(f\"⚠️ Server reported game over on reset attempt {attempt+1}. Retrying...\")\n",
    "                    if attempt < 2: # Reconnect if not last attempt\n",
    "                         self._connect_socket()\n",
    "                         time.sleep(0.5) # Small delay before retry\n",
    "                         continue # Retry the loop\n",
    "                    else:\n",
    "                         write_log(\"❌ Server still terminated after multiple reset attempts. Cannot proceed.\")\n",
    "                         raise RuntimeError(\"Tetris server failed to reset properly.\")\n",
    "                # Reset successful\n",
    "                self.lines_removed = 0\n",
    "                self.current_height = height\n",
    "                self.current_holes = holes\n",
    "                self.lifetime = 0\n",
    "                self.last_observation = observation.copy()\n",
    "                # write_log(f\"🔄 Environment Reset. Initial state: H={height}, O={holes}\") # Less verbose logging\n",
    "                info = {}\n",
    "                return observation, info\n",
    "\n",
    "            except (ConnectionAbortedError, ConnectionError, socket.error, TimeoutError) as e:\n",
    "                 write_log(f\"🔌 Connection issue during reset attempt {attempt+1} ({e}). Retrying...\")\n",
    "                 if attempt < 2:\n",
    "                      try:\n",
    "                          self._connect_socket() # Attempt reconnect\n",
    "                          time.sleep(0.5)\n",
    "                      except ConnectionError:\n",
    "                           write_log(\"   Reconnect failed.\")\n",
    "                           if attempt == 1: # If second attempt also fails, raise\n",
    "                               raise RuntimeError(f\"Failed to reconnect and reset Tetris server after multiple attempts: {e}\")\n",
    "                 else: # Final attempt failed\n",
    "                     raise RuntimeError(f\"Failed to reset Tetris server after multiple attempts: {e}\")\n",
    "\n",
    "        # Should not be reached if logic is correct, but as fallback:\n",
    "        raise RuntimeError(\"Failed to reset Tetris server.\")\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        self._initialize_pygame() # Ensure pygame is ready if in human mode\n",
    "\n",
    "        if self.render_mode == \"human\" and self.is_pygame_initialized:\n",
    "            import pygame\n",
    "            if self.window_surface is None:\n",
    "                 # This should not happen if _initialize_pygame worked, but handle defensively\n",
    "                 write_log(\"⚠️ Render called but Pygame window is not initialized.\")\n",
    "                 return\n",
    "\n",
    "            if hasattr(self, 'last_raw_render_frame'):\n",
    "                try:\n",
    "                    # last_raw_render_frame is (H, W, C) BGR from OpenCV\n",
    "                    render_frame_rgb = cv2.cvtColor(self.last_raw_render_frame, cv2.COLOR_BGR2RGB)\n",
    "                    # Pygame surface requires (width, height)\n",
    "                    surf = pygame.Surface((self.RESIZED_DIM, self.RESIZED_DIM))\n",
    "                    # Transpose needed: (H, W, C) -> (W, H, C) for Pygame surfarray\n",
    "                    pygame.surfarray.blit_array(surf, np.transpose(render_frame_rgb, (1, 0, 2)))\n",
    "                    # Scale up to window size\n",
    "                    surf = pygame.transform.scale(surf, self.window_surface.get_size())\n",
    "                    self.window_surface.blit(surf, (0, 0))\n",
    "                    pygame.event.pump() # Process internal Pygame events\n",
    "                    pygame.display.flip() # Update the full screen surface\n",
    "                    self.clock.tick(self.metadata[\"render_fps\"]) # Control frame rate\n",
    "                except Exception as e:\n",
    "                    write_log(f\"⚠️ Error during Pygame rendering: {e}\")\n",
    "                    # Attempt to close pygame gracefully on error?\n",
    "                    # self.close()\n",
    "\n",
    "            else:\n",
    "                # Draw a black screen if no frame available yet\n",
    "                 self.window_surface.fill((0, 0, 0))\n",
    "                 pygame.display.flip()\n",
    "\n",
    "        elif self.render_mode == \"rgb_array\":\n",
    "             if hasattr(self, 'last_raw_render_frame'):\n",
    "                 # Return RGB (H, W, C)\n",
    "                 return cv2.cvtColor(self.last_raw_render_frame, cv2.COLOR_BGR2RGB)\n",
    "             else:\n",
    "                 # Return black frame if no observation yet\n",
    "                 return np.zeros((self.RESIZED_DIM, self.RESIZED_DIM, 3), dtype=np.uint8)\n",
    "\n",
    "    def close(self):\n",
    "        # write_log(\"🔌 Closing environment connection.\") # Less verbose\n",
    "        if self.client_sock:\n",
    "            try:\n",
    "                self.client_sock.close()\n",
    "            except socket.error as e:\n",
    "                 write_log(f\"   Error closing socket: {e}\")\n",
    "            self.client_sock = None\n",
    "\n",
    "        if self.is_pygame_initialized:\n",
    "            try:\n",
    "                import pygame\n",
    "                pygame.display.quit()\n",
    "                pygame.quit()\n",
    "                self.is_pygame_initialized = False\n",
    "                # write_log(\"   Pygame window closed.\") # Less verbose\n",
    "            except Exception as e:\n",
    "                 write_log(f\"   Error closing Pygame: {e}\")\n",
    "\n",
    "# --- Environment Setup ---\n",
    "write_log(\"✅ 建立基礎環境函數 make_env...\")\n",
    "def make_env():\n",
    "    \"\"\"Helper function to create an instance of the Tetris environment.\"\"\"\n",
    "    env = TetrisEnv()\n",
    "    return env\n",
    "\n",
    "write_log(\"✅ 建立向量化環境 (DummyVecEnv)...\")\n",
    "# Use DummyVecEnv for single environment interaction\n",
    "train_env_base = DummyVecEnv([make_env])\n",
    "\n",
    "write_log(\"✅ 包裝環境 (VecFrameStack)...\")\n",
    "# Wrap with VecFrameStack (channel-first is important)\n",
    "# Use wandb config if available, otherwise use default\n",
    "n_stack = run.config[\"n_stack\"] if run else 4\n",
    "train_env_stacked = VecFrameStack(train_env_base, n_stack=n_stack, channels_order=\"first\")\n",
    "\n",
    "write_log(\"✅ 包裝環境 (VecNormalize - Rewards Only)...\")\n",
    "# Wrap with VecNormalize, NORMALIZING REWARDS ONLY.\n",
    "# Use wandb config if available, otherwise use default\n",
    "gamma = run.config[\"gamma\"] if run else 0.99\n",
    "train_env = VecNormalize(train_env_stacked, norm_obs=False, norm_reward=True, gamma=gamma)\n",
    "\n",
    "write_log(\"   環境建立完成並已包裝 (DummyVecEnv -> VecFrameStack -> VecNormalize)\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# DQN Model Setup and Training\n",
    "# ----------------------------\n",
    "write_log(\"🧠 設定 DQN 模型...\")\n",
    "# Use wandb config for hyperparameters if available, otherwise use defaults\n",
    "policy_type = run.config[\"policy_type\"] if run else \"CnnPolicy\"\n",
    "learning_rate = run.config[\"learning_rate\"] if run else 1e-4\n",
    "buffer_size = run.config[\"buffer_size\"] if run else 100000\n",
    "learning_starts = run.config[\"learning_starts\"] if run else 10000\n",
    "batch_size = run.config[\"batch_size\"] if run else 32\n",
    "tau = 1.0 # Default for DQN\n",
    "target_update_interval = run.config[\"target_update_interval\"] if run else 10000\n",
    "gradient_steps = 1 # Default for DQN\n",
    "exploration_fraction = run.config[\"exploration_fraction\"] if run else 0.1 # Default DQN explore fraction is smaller\n",
    "exploration_final_eps = run.config[\"exploration_final_eps\"] if run else 0.05\n",
    "\n",
    "# Define DQN model\n",
    "model = DQN(\n",
    "    policy=policy_type,\n",
    "    env=train_env,\n",
    "    verbose=1,\n",
    "    gamma=gamma,\n",
    "    learning_rate=learning_rate,\n",
    "    buffer_size=buffer_size,\n",
    "    learning_starts=learning_starts,\n",
    "    batch_size=batch_size,\n",
    "    tau=tau,\n",
    "    train_freq=(1, \"step\"), # Train every step\n",
    "    gradient_steps=gradient_steps,\n",
    "    target_update_interval=target_update_interval,\n",
    "    exploration_fraction=exploration_fraction,\n",
    "    exploration_final_eps=exploration_final_eps,\n",
    "    policy_kwargs=dict(normalize_images=False), # As per original code\n",
    "    seed=42, # Set seed for reproducibility\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    tensorboard_log=f\"/kaggle/working/runs/{run_id}\" if wandb_enabled else None # Log TB only if wandb enabled\n",
    ")\n",
    "write_log(f\"   模型建立完成. Device: {model.device}\")\n",
    "if run: write_log(f\"   使用 Wandb 超參數: {run.config}\")\n",
    "else: write_log(\"   使用默認超參數 (Wandb 未啟用).\")\n",
    "\n",
    "\n",
    "# Setup Wandb callback if enabled\n",
    "if wandb_enabled:\n",
    "    wandb_callback = WandbCallback(\n",
    "        gradient_save_freq=10000,\n",
    "        model_save_path=f\"/kaggle/working/models/{run_id}\",\n",
    "        model_save_freq=50000,\n",
    "        log=\"all\",\n",
    "        verbose=2\n",
    "    )\n",
    "    callback_list = [wandb_callback]\n",
    "else:\n",
    "    callback_list = None # No callback if wandb is disabled\n",
    "\n",
    "# --- Training ---\n",
    "write_log(f\"🚀 開始訓練 {TOTAL_TIMESTEPS} 步...\")\n",
    "training_successful = False\n",
    "try:\n",
    "    model.learn(\n",
    "        total_timesteps=TOTAL_TIMESTEPS,\n",
    "        callback=callback_list,\n",
    "        log_interval=10 # Log basic stats every 10 episodes\n",
    "    )\n",
    "    write_log(\"✅ 訓練完成!\")\n",
    "    training_successful = True\n",
    "except Exception as e:\n",
    "     write_log(f\"❌ 訓練過程中發生錯誤: {e}\", exc_info=True) # Log exception info\n",
    "     # Save model before exiting if error occurs mid-training\n",
    "     error_save_path = f'/kaggle/working/{STUDENT_ID}_dqn_error_save.zip'\n",
    "     try:\n",
    "        model.save(error_save_path)\n",
    "        write_log(f\"   模型已嘗試儲存至 {error_save_path}\")\n",
    "        if wandb_enabled: wandb.save(error_save_path)\n",
    "     except Exception as save_e:\n",
    "         write_log(f\"   ❌ 儲存錯誤模型時也發生錯誤: {save_e}\")\n",
    "     if run: run.finish(exit_code=1, quiet=True) # Finish wandb run with error code\n",
    "\n",
    "# --- Save Final Model (only if training completed successfully) ---\n",
    "if training_successful:\n",
    "    stats_path = f\"/kaggle/working/vecnormalize_stats_{run_id}.pkl\"\n",
    "    final_model_name = f'{STUDENT_ID}_dqn_final_{run_id}.zip'\n",
    "    final_model_path = os.path.join(\"/kaggle/working\", final_model_name)\n",
    "\n",
    "    try:\n",
    "        train_env.save(stats_path)\n",
    "        write_log(f\"   VecNormalize 統計數據已儲存至 {stats_path}\")\n",
    "        if wandb_enabled: wandb.save(stats_path)\n",
    "\n",
    "        model.save(final_model_path)\n",
    "        write_log(f\"✅ 最終模型已儲存: {final_model_path}\")\n",
    "        display(FileLink(final_model_path))\n",
    "        if wandb_enabled: wandb.save(final_model_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        write_log(f\"❌ 儲存最終模型或統計數據時出錯: {e}\")\n",
    "        training_successful = False # Mark as unsuccessful if saving fails\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation (only if training and saving were successful)\n",
    "# ----------------------------\n",
    "if training_successful:\n",
    "    write_log(\"\\n🧪 開始評估訓練後的模型...\")\n",
    "\n",
    "    # Create a separate evaluation environment\n",
    "    try:\n",
    "        eval_env_base = DummyVecEnv([make_env])\n",
    "\n",
    "        # Wrap with FrameStack FIRST, same as training\n",
    "        n_stack_eval = run.config[\"n_stack\"] if run else 4\n",
    "        eval_env_stacked = VecFrameStack(eval_env_base, n_stack=n_stack_eval, channels_order=\"first\")\n",
    "\n",
    "        # Load the SAME VecNormalize statistics\n",
    "        eval_env = VecNormalize.load(stats_path, eval_env_stacked)\n",
    "        eval_env.training = False\n",
    "        eval_env.norm_reward = False # IMPORTANT: দেখতে আসল reward\n",
    "\n",
    "        write_log(\"   評估環境建立成功.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        write_log(f\"❌ 錯誤: VecNormalize 統計文件未找到於 {stats_path}。跳過評估。\")\n",
    "        eval_env = None\n",
    "    except Exception as e:\n",
    "        write_log(f\"❌ 建立評估環境時出錯: {e}\")\n",
    "        eval_env = None\n",
    "\n",
    "    if eval_env is not None:\n",
    "        # --- Run Evaluation Episodes ---\n",
    "        num_eval_episodes = 5\n",
    "        total_rewards = []\n",
    "        total_lines = []\n",
    "        total_lifetimes = []\n",
    "        all_frames = []\n",
    "\n",
    "        try:\n",
    "            for i in range(num_eval_episodes):\n",
    "                obs = eval_env.reset()\n",
    "                done = False\n",
    "                episode_reward = 0\n",
    "                episode_lines = 0\n",
    "                episode_lifetime = 0\n",
    "                frames = []\n",
    "                last_info = {}\n",
    "\n",
    "                while not done:\n",
    "                    # Render base env for GIF\n",
    "                    try:\n",
    "                         base_env = eval_env.get_attr(\"envs\")[0].env\n",
    "                         raw_frame = base_env.render(mode=\"rgb_array\")\n",
    "                         if i == 0: frames.append(raw_frame) # Only for first ep\n",
    "                    except Exception as render_err:\n",
    "                         write_log(f\"⚠️ 評估時獲取渲染幀出錯: {render_err}\")\n",
    "\n",
    "                    # Predict and step\n",
    "                    action, _ = model.predict(obs, deterministic=True)\n",
    "                    obs, reward, done, infos = eval_env.step(action)\n",
    "\n",
    "                    episode_reward += reward[0]\n",
    "                    last_info = infos[0]\n",
    "                    # Use .get() for safety, default to previous value if key missing\n",
    "                    episode_lines = last_info.get('removed_lines', episode_lines)\n",
    "                    episode_lifetime = last_info.get('lifetime', episode_lifetime)\n",
    "                    done = done[0]\n",
    "\n",
    "                write_log(f\"   評估 Episode {i+1}: Reward={episode_reward:.2f}, Lines={episode_lines}, Steps={episode_lifetime}\")\n",
    "                total_rewards.append(episode_reward)\n",
    "                total_lines.append(episode_lines)\n",
    "                total_lifetimes.append(episode_lifetime)\n",
    "                if i == 0: all_frames = frames\n",
    "\n",
    "            write_log(f\"--- 評估結果 ({num_eval_episodes} episodes) ---\")\n",
    "            mean_reward = np.mean(total_rewards)\n",
    "            std_reward = np.std(total_rewards)\n",
    "            mean_lines = np.mean(total_lines)\n",
    "            std_lines = np.std(total_lines)\n",
    "            mean_lifetime = np.mean(total_lifetimes)\n",
    "            std_lifetime = np.std(total_lifetimes)\n",
    "\n",
    "            write_log(f\"   平均 Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "            write_log(f\"   平均 Lines: {mean_lines:.2f} +/- {std_lines:.2f}\")\n",
    "            write_log(f\"   平均 Steps: {mean_lifetime:.2f} +/- {std_lifetime:.2f}\")\n",
    "\n",
    "            # Log evaluation metrics to Wandb\n",
    "            if wandb_enabled:\n",
    "                wandb.log({\n",
    "                    \"eval/mean_reward\": mean_reward, \"eval/std_reward\": std_reward,\n",
    "                    \"eval/mean_lines\": mean_lines, \"eval/std_lines\": std_lines,\n",
    "                    \"eval/mean_lifetime\": mean_lifetime, \"eval/std_lifetime\": std_lifetime,\n",
    "                })\n",
    "\n",
    "            # --- Generate Replay GIF ---\n",
    "            if all_frames:\n",
    "                gif_path = f'/kaggle/working/replay_eval_{run_id}.gif'\n",
    "                write_log(f\"💾 正在儲存評估回放 GIF 至 {gif_path}...\")\n",
    "                try:\n",
    "                    imageio.mimsave(gif_path, [np.array(frame).astype(np.uint8) for frame in all_frames], fps=15, loop=0)\n",
    "                    write_log(\"   GIF 儲存成功.\")\n",
    "                    display(FileLink(gif_path))\n",
    "                    if wandb_enabled: wandb.log({\"eval/replay\": wandb.Video(gif_path, fps=15, format=\"gif\")})\n",
    "                except Exception as e: write_log(f\"   ❌ 儲存 GIF 時發生錯誤: {e}\")\n",
    "            else: write_log(\"   ⚠️ 未能儲存 GIF (沒有收集到幀).\")\n",
    "\n",
    "            # --- Save Evaluation Results CSV ---\n",
    "            csv_filename = f'tetris_evaluation_scores_{run_id}.csv'\n",
    "            csv_path = os.path.join(\"/kaggle/working\", csv_filename)\n",
    "            try:\n",
    "                with open(csv_path, 'w') as fs:\n",
    "                    fs.write('episode_id,removed_lines,played_steps,reward\\n')\n",
    "                    if total_lines: # Ensure lists are not empty\n",
    "                        fs.write(f'eval_0,{total_lines[0]},{total_lifetimes[0]},{total_rewards[0]:.2f}\\n')\n",
    "                    fs.write(f'eval_avg,{mean_lines:.2f},{mean_lifetime:.2f},{mean_reward:.2f}\\n')\n",
    "                write_log(f\"✅ 評估分數 CSV 已儲存: {csv_path}\")\n",
    "                display(FileLink(csv_path))\n",
    "                if wandb_enabled: wandb.save(csv_path)\n",
    "            except Exception as e: write_log(f\"   ❌ 儲存 CSV 時發生錯誤: {e}\")\n",
    "\n",
    "        except Exception as eval_e:\n",
    "            write_log(f\"❌ 評估迴圈中發生錯誤: {eval_e}\", exc_info=True)\n",
    "\n",
    "        finally:\n",
    "             # Ensure evaluation env is closed even if errors occur\n",
    "             if eval_env:\n",
    "                 eval_env.close()\n",
    "                 write_log(\"   評估環境已關閉.\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "write_log(\"🧹 清理環境...\")\n",
    "if 'train_env' in locals() and train_env: # Check if train_env exists\n",
    "    train_env.close()\n",
    "    write_log(\"   訓練環境已關閉.\")\n",
    "# Close the Java server process\n",
    "if java_process and java_process.poll() is None:\n",
    "     write_log(\"   正在終止 Java server process...\")\n",
    "     java_process.terminate()\n",
    "     try:\n",
    "         java_process.wait(timeout=5)\n",
    "         write_log(\"   Java server process 已終止.\")\n",
    "     except subprocess.TimeoutExpired:\n",
    "         write_log(\"   Java server 未能在 5 秒內終止, 強制結束...\")\n",
    "         java_process.kill()\n",
    "         write_log(\"   Java server process 已強制結束.\")\n",
    "elif java_process and java_process.poll() is not None:\n",
    "     write_log(\"   Java server process 已自行結束.\")\n",
    "else:\n",
    "     write_log(\"   Java server process 未啟動或已關閉.\")\n",
    "\n",
    "\n",
    "# Finish the Wandb run if it was initialized and training didn't crash early\n",
    "if run:\n",
    "    if training_successful:\n",
    "         run.finish()\n",
    "         write_log(\"✨ Wandb run finished.\")\n",
    "    else:\n",
    "         # Run might have already been finished in the exception handler\n",
    "         if run.is_running:\n",
    "              run.finish(exit_code=1) # Ensure it's marked as failed\n",
    "         write_log(\"✨ Wandb run finished (marked as failed due to error).\")\n",
    "\n",
    "write_log(\"🏁 腳本執行完畢.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11461423,
     "sourceId": 96443,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
